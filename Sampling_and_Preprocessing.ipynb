{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Assignment #3: Streaming Data Insights\n",
    "\n",
    "### Frequent Itemset Analysis on Amazon Metadata\n",
    "\n",
    "## Data Sampling and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group members:\n",
    "\n",
    "- Aaqib Ahmed Nazir (i22-1920),\n",
    "- Arhum Khan (i22-1967),\n",
    "- Ammar Khasif (i22-1968)\n",
    "\n",
    "##### Section: DS-D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries Used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import itertools \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Data\n",
    "\n",
    "#### Extracting a 15GB sample from the original 105GB dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_json(input_file, output_file, target_size_gb, filter_key='also_buy'):\n",
    "    target_size_bytes = target_size_gb * 1024**3\n",
    "    current_size_bytes = 0\n",
    "\n",
    "    # Reading the input file and writing the output file\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in tqdm(infile, desc=f\"Processing {input_file}\"):\n",
    "            record = json.loads(line)\n",
    "            # Filtering out records\n",
    "            if record.get(filter_key):\n",
    "                outfile.write(json.dumps(record) + '\\n')\n",
    "                current_size_bytes += len(line.encode('utf-8'))\n",
    "\n",
    "            if current_size_bytes >= target_size_bytes:\n",
    "                break\n",
    "\n",
    "    print(f\"Finished sampling. Output size: {current_size_bytes / 1024**3:.2f} GB\")\n",
    "\n",
    "sample_json('H:\\\\All_Amazon_Meta.json', 'Sampled_Amazon_Meta.json', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing JSON Errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_json_errors(line):\n",
    "    # Removing extra commas\n",
    "    line = line.strip()\n",
    "    line = line.rstrip(\",\")\n",
    "    # Adding missing closing brackets\n",
    "    if not line.endswith(\"}\") and not line.endswith(\"]\"):\n",
    "        if \"{\" in line and \"}\" not in line:\n",
    "            line += \"}\"\n",
    "        elif \"[\" in line and \"]\" not in line:\n",
    "            line += \"]\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the JSON errors and saving the fixed data in a new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line_by_line(input_path, output_path):\n",
    "    errors_count = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file, open(\n",
    "        output_path, \"w\", encoding=\"utf-8\"\n",
    "    ) as outfile:\n",
    "        # Process the file line by line\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            try:\n",
    "                corrected_line = fix_common_json_errors(line)\n",
    "                data = json.loads(corrected_line)\n",
    "                # Writing fixed JSON object to the file\n",
    "                json.dump(data, outfile)\n",
    "                outfile.write(\"\\n\")  \n",
    "            except json.JSONDecodeError as e:\n",
    "                errors_count += 1\n",
    "                print(f\"Error in line {line_number}: {e}\")\n",
    "    print(f\"Finished processing. Total errors: {errors_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"Sampled_Amazon_Meta.json\"\n",
    "output_file_path = \"Corrected_Sample_Amazon_Meta.json\"\n",
    "\n",
    "process_line_by_line(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Cleaning the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unwanted HTML tags, URLs, and normalizing the whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category', 'tech1', 'description', 'fit', 'title', 'also_buy', 'image', 'tech2', 'brand', 'feature', 'rank', 'also_view', 'details', 'main_cat', 'similar_item', 'date', 'price', 'asin']\n"
     ]
    }
   ],
   "source": [
    "# loading only the column names from the sampled file\n",
    "input_file = \"H:\\\\Sampled_Amazon_Meta.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    first_line = infile.readline()\n",
    "    column_names = list(json.loads(first_line).keys())\n",
    "\n",
    "# displaying the column names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    unwanted_patterns = [\n",
    "        r\"<[^>]*>\",  # HTML tags\n",
    "        r\"https?:\\/\\/\\S+\",  # URLs\n",
    "        r\"P\\.when\\(.*?\\);\",  # JS snippets\n",
    "        r\"span class\\w+\",  # span classes\n",
    "    ]\n",
    "\n",
    "    if any(re.search(pattern, text) for pattern in unwanted_patterns):\n",
    "        return \"\"\n",
    "\n",
    "    # Removing back slashes, extra whitespaces, and punctuation\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and cleaning the relevant info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item_data(item_data):\n",
    "    # Extracting selected columns\n",
    "    asin = item_data.get(\"asin\", \"\")\n",
    "    title = clean_text(item_data.get(\"title\", \"No Title Available\"))\n",
    "    brand = item_data.get(\"brand\", \"Unknown Brand\")\n",
    "    categories = item_data.get(\"categories\", [])\n",
    "\n",
    "    # Extracting related products\n",
    "    related_products = set(item_data.get(\"also_buy\", [])) | set(\n",
    "        item_data.get(\"also_viewed\", [])\n",
    "    )\n",
    "    related = list(related_products) if related_products else []\n",
    "\n",
    "    # Preparing preprocessed item data with selected cols\n",
    "    preprocessed_item = {\n",
    "        \"asin\": asin,\n",
    "        \"title\": title,\n",
    "        \"brand\": brand,\n",
    "        \"categories\": categories,\n",
    "        \"related\": related,\n",
    "    }\n",
    "\n",
    "    return preprocessed_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"H:\\\\Sampled_Amazon_Meta.json\"\n",
    "output_file_path = \"preprocessed_dataset1.json\"\n",
    "\n",
    "item_data_collection = []\n",
    "with open(input_file_path, \"r\") as file_input:\n",
    "    for line in file_input:\n",
    "        try:\n",
    "            raw_data = json.loads(line)\n",
    "            processed_item = process_item_data(raw_data)\n",
    "            item_data_collection.append(processed_item)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON from line: {line}\")\n",
    "\n",
    "# Write all processed data to a JSON file\n",
    "with open(output_file_path, \"w\") as file_output:\n",
    "    json.dump(item_data_collection, file_output, indent=4)\n",
    "    \n",
    "print(\"Dataset processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>related</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6342509379</td>\n",
       "      <td>QIBOE Mens Baggy Jeans Denim Sweatpants Loose ...</td>\n",
       "      <td>[B071PFP967, B010V0WTP2, B0156SZQ5O, B01FVRKZ4...</td>\n",
       "      <td>QIBOE</td>\n",
       "      <td>[Clothing, Shoes &amp; Jewelry, Men, Clothing, Jea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6342502315</td>\n",
       "      <td>Crazy Womens Voile Crinkle Scarf Shawl</td>\n",
       "      <td>[B01LLOUFRQ, B01LYDMB6U, B019ZAYUB0, B00NV1VFP...</td>\n",
       "      <td>Crazy</td>\n",
       "      <td>[Clothing, Shoes &amp; Jewelry, Women, Accessories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6342522545</td>\n",
       "      <td>FQQ Women Sexy Lingerie Lace Dress Sheer Babyd...</td>\n",
       "      <td>[B00VBVXVPI]</td>\n",
       "      <td>FQQ</td>\n",
       "      <td>[Clothing, Shoes &amp; Jewelry, Women, Clothing, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6342522898</td>\n",
       "      <td>Crazy Womens Sexy Leather Backless Bodycon Clu...</td>\n",
       "      <td>[B07219C7LQ, B015W134LS, B06ZZBQMT4, B01AHZSZ9...</td>\n",
       "      <td>Crazy</td>\n",
       "      <td>[Clothing, Shoes &amp; Jewelry, Women, Clothing, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6342523002</td>\n",
       "      <td>FQQ Womens Sexy Lingerie Babydoll Dress Sleepw...</td>\n",
       "      <td>[B0723CQH2L, B01LY4VKTL, B06XKWCGTT, B074Z3QGM...</td>\n",
       "      <td>FQQ</td>\n",
       "      <td>[Clothing, Shoes &amp; Jewelry, Women, Clothing, L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              title  \\\n",
       "0  6342509379  QIBOE Mens Baggy Jeans Denim Sweatpants Loose ...   \n",
       "1  6342502315             Crazy Womens Voile Crinkle Scarf Shawl   \n",
       "2  6342522545  FQQ Women Sexy Lingerie Lace Dress Sheer Babyd...   \n",
       "3  6342522898  Crazy Womens Sexy Leather Backless Bodycon Clu...   \n",
       "4  6342523002  FQQ Womens Sexy Lingerie Babydoll Dress Sleepw...   \n",
       "\n",
       "                                             related  brand  \\\n",
       "0  [B071PFP967, B010V0WTP2, B0156SZQ5O, B01FVRKZ4...  QIBOE   \n",
       "1  [B01LLOUFRQ, B01LYDMB6U, B019ZAYUB0, B00NV1VFP...  Crazy   \n",
       "2                                       [B00VBVXVPI]    FQQ   \n",
       "3  [B07219C7LQ, B015W134LS, B06ZZBQMT4, B01AHZSZ9...  Crazy   \n",
       "4  [B0723CQH2L, B01LY4VKTL, B06XKWCGTT, B074Z3QGM...    FQQ   \n",
       "\n",
       "                                          categories  \n",
       "0  [Clothing, Shoes & Jewelry, Men, Clothing, Jea...  \n",
       "1  [Clothing, Shoes & Jewelry, Women, Accessories...  \n",
       "2  [Clothing, Shoes & Jewelry, Women, Clothing, L...  \n",
       "3  [Clothing, Shoes & Jewelry, Women, Clothing, D...  \n",
       "4  [Clothing, Shoes & Jewelry, Women, Clothing, L...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Amazon_data = pd.read_json('preprocessed_dataset.json')\n",
    "\n",
    "display(Amazon_data.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
